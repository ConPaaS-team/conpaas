With the rise of cloud computing, modern enterprise software systems started to deploy their services over clouds. As a utility-oriented and service-based approach to computing, cloud infrastructures offer many attractive features to their customers.  In particular, cloud providers allow tenants to rent resources in a \emph{pay-as-you-go} fashion. This pricing model is specifically employed by enterprise software systems where the assurance of QoS requirements is crucial to boost the volume of customers, and hence their revenues. Typically these requirements are specified by the enterprise (customer) and affirmed by cloud provider in the form of a \emph{service level agreement} (SLA), and vary depending on the size of the enterprise. Thus, large enterprises such as Animoto, \emph{The guardian} and Spotify, pay more to provide high assurance of availability and performance to their clients \cite{casestudiescloud}; while small enterprises pay less to obtain an acceptable performance but with little service availability. 


%Application providers pay for server resources, and in turn, are provided performance guarantees, expressed in the form of a service level agrreement (SLA).

When dealing with websites, the fulfillment of the SLA requirements becomes more problematic, as the workload demand fluctuates as a result of sudden changes in the popularity and/or request mix, flash crowds, outages and network misconfigurations.
For instance, on June 25th 2009, the news of Michael Jackson's death quickly crippled popular websites, such as \emph{TMZ.com}, \emph{The Angeles Times} or \emph{Twitter}, and resulted in hours-long slowdown or outages~\cite{outagesTimes}. These traffic anomalies are specifically difficult to handle by traditional resource management systems, thus causing long periods of violations of the SLA requirements~\cite{trafficCongestion}. Failure to comply with these requirements are often associated with significant financial penalties or other forms of loss of revenue such as decreases in the user base. 
Therefore, it is crucial to use resource provisioning systems that scale an application on demand while meeting the requirements.

Nowadays, cloud infrastructures provide elastic provisioning by supporting a variety of scaling mechanisms and diverse list of different hardware configurations for rent, each with a different infrastructure cost. Even though the diversity of hardware configurations is common in cloud infrastructures, the majority of resource provisioning systems focus on minimizing the infrastructure cost rather than selecting a right combination of resources~\cite{herbst_2013,urgaonkar_agile_2008,dejavu2012}. Moreover, when using these systems to scale in response to changing conditions, most of them restrict themselves to a single type of hardware configuration, ignoring the important avenues for cost/performance optimization. We believe the selection of multiple resources with different performance capacity/cost characteristics mitigate the degradations due to sudden workload demand fluctuations. As a consequence, a new challenge arises to autoscaling systems, as they have to decide which type of resources to choose for a particular workload. In the following, we use the term \emph{scaling plan} to refer to this searching process to find the most appropriate resource combination. 

%This manner to proceed may lead to periods of unsatisfactory performance, specifically for applications that need to provide high availability to their clients.  

Furthermore, traditional autoscaling systems do not allow to adapt the selection criteria of the scaling plan to customer preferences like service availability, cost or performance. From one customer to another, the tradeoff between cost and SLA fulfillment vary, especially when handling flash crowds or other traffic anomalies, so therefore autoscaling systems have to choose scaling plans adapted to the customer preferences. As an example, large enterprises willing to pay more will provision powerful resources to absorb traffic spikes, while small enterprises prefer to fine-tune their budgets provisioning cheapest resources that have less slack to handle any eventual spike. There is a necessary tradeoff between the cost one customer is ready to spend and the performance guarantee that no SLO violation will occur.

%As an example, while running an application, users may define a policy to add two additional resources when the load on the running resources reaches 75\%.  

%our concern here is to design an autoscaling system that mitigates these penalties.

%this system are not able to decide which resources configuration fits better with the current workload.

%The definition of a scaling plan according to the servide demand over the time is needed.

% App metrics instead of other system which hare limited to VM-level metrics. (Pluggable autoscaling service)
%Measuring instance performance without additional resource
%SLO penalty

%We believe that by exploiting the heterogeneity of cloud infrastructure the performance degradations caused by outages could be mitigated.

\textbf{Contribution.}  This paper presents an autoscaling system that benefits from the heterogeneity of cloud infrastructures to better enforce the customer requirements, even under large and temporary workload variations. The selection of an appropriate combination of resources provides enough computing capacity to handle the traffic variations without drastically raising the cost. To achieve that, our system profiles each type of allocated resources to measure their capacity, and in conjunction with a medium-term traffic predictor devises the \emph{scaling plan} that better match the workload requirements. In this system, each customer can tune its own cost/SLA fulfillment tradeoff thanks to the metal classification, which pre-defines different criteria for the selection of scaling plans based on the customer preferences. To handle resource heterogeneity, the proposed system provides a weighted load balancing mechanism that enables to distribute the incoming traffic across resources depending on their performance capacities. We evaluated our system in realistic unstable workload situations by deploying a copy of Wikipedia and replaying a fraction of the real access traces to its official web site. This evaluation was conducted on both private and public clouds using different cost/SLA fulfillment configurations.

% By doing so, we minimize the performance degradations caused by traffic spikes or other traffic anomalies.  and measures resource performance to provide more accurate scaling decisions. according to the workload requirements.

 %A use the to choose the best combination of resources according to the customer requirmetns. online profiling techniques

%When



\textbf{Outline.} Section~\ref{sec:motivation} identifies the problem statement and motivation; Section~\ref{sec:proposed_approach} presents our autoscaling system approach; Section~\ref{sec:evaluation} discusses our experimental evaluation; Section~\ref{sec:relatedWorks} reviews the related work; and Section~\ref{sec:conclusion} presents concluding remarks.