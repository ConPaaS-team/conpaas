%One of the major innovations provided by Cloud computing platforms is
%their pay-per-use model where clients pay only for the resources they
%actually use. This business model is particularly favorable for
%application domains where workloads vary widely over time, such as the
%domain of Web application hosting. 

%\paragraph{What is resource provisioning}
%Web applications in the cloud can
%request and release resources at any time according to their needs.

%\paragraph{Issues: when to provision, how to provision}
%However, provisioning the right volume of resources for a Web
%application is not a simple task. 

%\paragraph{Heterogeneous VM types}


%\paragraph{Cost/safety tradeoff}

%\paragraph{Contribution}

%\paragraph{Outline}

%Cloud computing is an attractive environment for hosting Web
%applications, and many major websites such as Animoto, The Guardian
%and Spotify have decided to move at least parts of their applications
%to the cloud~\cite{casestudiescloud}. Among the many reasons for such
%decisions is the ability of Cloud platforms to deliver arbitrary
%quantities of resources to applications to follow the demand imposed
%by end users. Such applications typically try to enforce a 


With the rise of cloud computing, modern enterprise software systems
started to deploy their services over clouds. As a utility-oriented
and service-based approach to computing, cloud infrastructures offer
many attractive features to their customers.  In particular, cloud
providers allow tenants to rent resources in a \emph{pay-as-you-go}
fashion. This pricing model is specifically employed by enterprise
software systems where the assurance of QoS requirements is crucial to
boost the volume of customers, and hence their revenues. Typically
these requirements are specified by the enterprise (customer) and
affirmed by cloud provider in the form of a \emph{service level
agreement} (SLA), and vary depending on the size of the
enterprise. Thus, large enterprises such as Animoto, \emph{The
guardian} and Spotify, pay more to provide high assurance of
availability and performance to their clients \cite{casestudiescloud};
while small enterprises pay less to obtain an acceptable performance
but with little service availability.


When dealing with websites, the fulfillment of the SLA requirements
becomes more problematic, as the workload demand fluctuates as a
result of sudden changes in the popularity and/or request mix, flash
crowds, outages and network misconfigurations.  For instance, on June
25th 2009, the news of Michael Jackson's death quickly crippled
popular websites, such as \emph{TMZ.com}, \emph{The Angeles Times}
or \emph{Twitter}, and resulted in hours-long slowdown or
outages~\cite{outagesTimes}. These sudden traffic fluctuations are
specifically difficult to handle by traditional resource management
systems, thus causing long periods of violations of the SLA
requirements~\cite{trafficCongestion}. Failure to comply with these
requirements are often associated with significant financial penalties
or other forms of loss of revenue such as decreases in the user base.
Therefore, it is crucial to use resource provisioning systems that
scale an application on demand while meeting the requirements.

Nowadays, cloud infrastructures provide elastic provisioning by
supporting a variety of scaling mechanisms and diverse list of
different hardware configurations for rent, each with a different
infrastructure cost. Even though the diversity of hardware
configurations is common in cloud infrastructures, the majority of
resource provisioning systems focus on minimizing the infrastructure
cost rather than selecting a right combination of
resources~\cite{herbst_2013,urgaonkar_agile_2008,dejavu2012}. Moreover,
when using these systems to scale in response to changing conditions,
most of them restrict themselves to a single type of hardware
configuration, ignoring the important avenues for cost/performance
optimization. We believe the selection of multiple resources with
different performance capacity/cost characteristics mitigate the
degradations due to sudden workload demand fluctuations. As a
consequence, a new challenge arises to autoscaling systems, as they
have to decide which type of resources to choose for a particular
workload. In the following, we use the term \emph{scaling plan} to
refer to this searching process to find the most appropriate resource
combination.

%This manner to proceed may lead to periods of unsatisfactory performance, specifically for applications that need to provide high availability to their clients.  

Traditional autoscaling systems do not allow to adapt the
selection criteria of scaling plans to customer preferences like
service availability, cost or performance. From one customer to
another, the tradeoff between cost and SLA fulfillment vary,
especially when handling flash crowds or other traffic anomalies.
Therefore autoscaling systems have to choose the scaling plan that
better match to the customer preferences. As an example, large
enterprises willing to pay more will provision powerful resources to
absorb traffic spikes, while small enterprises prefer to fine-tune
their budgets provisioning cheapest resources that have less slack to
handle any eventual spike. There is a necessary tradeoff between the
cost one customer is ready to spend and the performance guarantee that
no SLO (Service Level Objective) violation will occur.

%As an example, while running an application, users may define a policy to add two additional resources when the load on the running resources reaches 75\%.  

%our concern here is to design an autoscaling system that mitigates these penalties.

%this system are not able to decide which resources configuration fits better with the current workload.

%The definition of a scaling plan according to the servide demand over the time is needed.

% App metrics instead of other system which hare limited to VM-level metrics. (Pluggable autoscaling service)
%Measuring instance performance without additional resource
%SLO penalty

%We believe that by exploiting the heterogeneity of cloud infrastructure the performance degradations caused by outages could be mitigated.


This paper presents an autoscaling system that benefits from the
heterogeneity of cloud infrastructures to better enforce customer
requirements, even under large and temporary workload variations. The
selection of an appropriate combination of resources provides enough
computing capacity to handle the traffic variations without
drastically raising the cost. To achieve that, our system profiles
each type of allocated resources to measure their capacity, and in
conjunction with a medium-term traffic predictor devises
the \emph{scaling plan} that better match the workload
requirements. In this system, each customer can tune its own cost/SLA
fulfillment tradeoff thanks to the metal classification, which
pre-defines different criteria for the selection of scaling plans
based on the customer preferences. To handle resource heterogeneity,
the proposed system provides a weighted load balancing mechanism that
enables the distribution of the incoming traffic across resources depending
on their performance capacities. We evaluated our system in realistic
unstable workload situations by deploying a copy of Wikipedia and
replaying a fraction of the real access traces to its official web
site. This evaluation was conducted on both private and public clouds
using different cost/SLA fulfillment configurations.

This paper is organized as follows: Section~\ref{sec:motivation} identifies the problem statement and
motivation; Section~\ref{sec:proposed_approach} presents our
autoscaling system approach; Section~\ref{sec:decisionMaking} explains
how our system works; Section~\ref{sec:evaluation} discusses
our experimental evaluation; Section~\ref{sec:relatedWorks} reviews
the related work; and Section~\ref{sec:conclusion} presents concluding
remarks.
